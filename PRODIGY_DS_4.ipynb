{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25641733",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af9b2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(r\"C:\\Users\\LENOVO\\Desktop\\Data Science Internship\\archive.zip_04\\twitter_training.csv\")\n",
    "validation_data = pd.read_csv(r\"C:\\Users\\LENOVO\\Desktop\\Data Science Internship\\archive.zip_04\\twitter_validation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f791429",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a435f497",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14bd850",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc91a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1671a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows of both datasets\n",
    "train_head = train_data.head()\n",
    "validation_head = validation_data.head()\n",
    "\n",
    "train_head, validation_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab53054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_train = train_data.isnull().sum()\n",
    "missing_validation = validation_data.isnull().sum()\n",
    "\n",
    "# Check for duplicate rows\n",
    "duplicates_train = train_data.duplicated().sum()\n",
    "duplicates_validation = validation_data.duplicated().sum()\n",
    "\n",
    "missing_train, missing_validation, duplicates_train, duplicates_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffda3cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the distribution of sentiment labels\n",
    "train_sentiment_distribution = train_data.iloc[:, 2].value_counts()\n",
    "validation_sentiment_distribution = validation_data.iloc[:, 2].value_counts()\n",
    "\n",
    "# Checking the number of unique entities in the training set\n",
    "unique_entities_train = train_data.iloc[:, 1].nunique()\n",
    "\n",
    "train_sentiment_distribution, validation_sentiment_distribution, unique_entities_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328004b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.duplicated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564f4cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data.duplicated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cd1aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76acfe77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate rows from the training set\n",
    "train_data_cleaned = train_data.drop_duplicates()\n",
    "\n",
    "# Drop rows with missing tweet/message values\n",
    "train_data_cleaned = train_data_cleaned.dropna(subset=[train_data.columns[3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b911bda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the cleaning\n",
    "remaining_duplicates_train = train_data_cleaned.duplicated().sum()\n",
    "remaining_missing_train = train_data_cleaned.isnull().sum()\n",
    "\n",
    "remaining_duplicates_train, remaining_missing_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514220f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the plots\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "# Plot sentiment distribution for training data\n",
    "sns.countplot(data=train_data_cleaned, x=train_data_cleaned.columns[2], order=['Positive', 'Negative', 'Neutral', 'Irrelevant'], ax=ax[0])\n",
    "ax[0].set_title('Sentiment Distribution of Training Data')\n",
    "ax[0].set_ylabel('Count')\n",
    "ax[0].set_xlabel('Sentiment')\n",
    "\n",
    "# Plot sentiment distribution for validation data\n",
    "sns.countplot(data=validation_data, x=validation_data.columns[2], order=['Positive', 'Negative', 'Neutral', 'Irrelevant'], ax=ax[1])\n",
    "ax[1].set_title('Sentiment Distribution of Validation Data')\n",
    "ax[1].set_ylabel('Count')\n",
    "ax[1].set_xlabel('Sentiment')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760a5814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the length of each message\n",
    "train_data_cleaned['message_length'] = train_data_cleaned[train_data_cleaned.columns[3]].apply(len)\n",
    "validation_data['message_length'] = validation_data[validation_data.columns[3]].apply(len)\n",
    "\n",
    "# Set up the plots\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot message length distribution for training data\n",
    "sns.histplot(train_data_cleaned['message_length'], bins=50, ax=ax[0], color='orange')\n",
    "ax[0].set_title('Message Length Distribution in Training Data')\n",
    "ax[0].set_ylabel('Frequency')\n",
    "ax[0].set_xlabel('Message Length')\n",
    "\n",
    "# Plot message length distribution for validation data\n",
    "sns.histplot(validation_data['message_length'], bins=50, ax=ax[1], color='grey')\n",
    "ax[1].set_title('Message Length Distribution in Validation Data')\n",
    "ax[1].set_ylabel('Frequency')\n",
    "ax[1].set_xlabel('Message Length')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3da2662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the word cloud for the entire training dataset\n",
    "all_text = \" \".join(tweet for tweet in train_data_cleaned[train_data_cleaned.columns[3]])\n",
    "wordcloud_all = WordCloud(background_color='black', width=800, height=400).generate(all_text)\n",
    "\n",
    "# Plot the word cloud\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.imshow(wordcloud_all, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Word Cloud for All Tweets in Training Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58dcfc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize sentiment categories\n",
    "sentiments = ['Positive', 'Negative', 'Neutral', 'Irrelevant']\n",
    "\n",
    "# Set up the plots\n",
    "fig, axs = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Generate and plot word clouds for each sentiment\n",
    "for sentiment, ax in zip(sentiments, axs.ravel()):\n",
    "    sentiment_text = \" \".join(tweet for tweet in train_data_cleaned[train_data_cleaned[train_data_cleaned.columns[2]] == sentiment][train_data_cleaned.columns[3]])\n",
    "    wordcloud_sentiment = WordCloud(background_color='black', width=400, height=200).generate(sentiment_text)\n",
    "    \n",
    "    ax.imshow(wordcloud_sentiment, interpolation='bilinear')\n",
    "    ax.axis('off')\n",
    "    ax.set_title(f'Word Cloud for {sentiment} Sentiment')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa36ed95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text_simplified(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Simple tokenization using split (without relying on NLTK)\n",
    "    tokens = text.split()\n",
    "    # Remove special characters and numbers\n",
    "    tokens = [token for token in tokens if token.isalpha()]\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03b8af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_cleaned['processed_message_simplified'] = train_data_cleaned[train_data_cleaned.columns[3]].apply(preprocess_text_simplified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f387c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusting the labels: Convert \"Irrelevant\" labels to \"Neutral\"\n",
    "train_data_cleaned[train_data_cleaned.columns[2]] = train_data_cleaned[train_data_cleaned.columns[2]].replace('Irrelevant', 'Neutral')\n",
    "validation_data[validation_data.columns[2]] = validation_data[validation_data.columns[2]].replace('Irrelevant', 'Neutral')\n",
    "\n",
    "# Check the updated sentiment distribution in the training and validation data\n",
    "updated_train_sentiment_distribution = train_data_cleaned[train_data_cleaned.columns[2]].value_counts()\n",
    "updated_validation_sentiment_distribution = validation_data[validation_data.columns[2]].value_counts()\n",
    "\n",
    "updated_train_sentiment_distribution, updated_validation_sentiment_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ef1645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract text for Positive sentiment from the training dataset\n",
    "positive_text = \" \".join(tweet for tweet in train_data_cleaned[train_data_cleaned[train_data_cleaned.columns[2]] == 'Positive']['processed_message_simplified'])\n",
    "\n",
    "# Generate word cloud for Positive sentiment\n",
    "wordcloud_positive = WordCloud(background_color='black', width=800, height=400).generate(positive_text)\n",
    "\n",
    "# Plot the word cloud\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud_positive, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Word Cloud for Positive Sentiment in Training Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cbda5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize the TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)  # Limiting to 5000 features for computational efficiency\n",
    "\n",
    "# Fit and transform the preprocessed text from the training data\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(train_data_cleaned['processed_message_simplified'])\n",
    "\n",
    "# Apply simplified preprocessing to the validation data\n",
    "validation_data['processed_message_simplified'] = validation_data[validation_data.columns[3]].apply(preprocess_text_simplified)\n",
    "\n",
    "# Transform the preprocessed text from the validation data\n",
    "X_validation_tfidf = tfidf_vectorizer.transform(validation_data['processed_message_simplified'])\n",
    "\n",
    "# Extract target labels for training and validation\n",
    "y_train = train_data_cleaned[train_data_cleaned.columns[2]]\n",
    "y_validation = validation_data[validation_data.columns[2]]\n",
    "X_train_tfidf.shape, X_validation_tfidf.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
